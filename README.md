# AlgoTrade 2025 - El Clasico Team

A sophisticated algorithmic trading system that uses **Put-Call Parity** signals to drive its core trading strategy, supplemented by **Predictive Market Models** for financial derivatives trading.

## üöÄ Key Features

- **Hybrid Trading Strategy**: Implements a strategy where Put-Call Parity violations are used as a signal to execute a two-legged trade (buying a future and a put option).
- **Predictive Modeling**: Utilizes a LightGBM-based pipeline to forecast asset prices with uncertainty quantification.
- **Real-Time Trading**: Features a robust WebSocket-based engine for live data processing and order management.
- **Comprehensive Tooling**: Includes scripts for data collection, feature engineering, model training, and evaluation.

## üìä Trading Strategy: Put-Call Parity Signal

The primary trading strategy, implemented in `src/bots/main_bot.py`, uses the Put-Call Parity principle as a signal generator for placing trades. It is a form of statistical arbitrage rather than a pure, risk-free arbitrage.

**Core Logic**:
1.  **Real-Time Pricing**: The bot continuously calculates weighted average prices for futures, calls, and puts from the live order book.
2.  **Synthetic Future Calculation**: It uses the Put-Call Parity formula (`Call Price - Put Price + Strike Price`) to calculate a "synthetic" future price for each options series.
3.  **Signal Detection**: A trading signal is generated when the synthetic future price is significantly more expensive (by a configurable threshold, e.g., >2.25%) than the actual traded future's price.
4.  **Trade Execution**: Upon detecting a signal, the bot executes a specific two-leg trade:
    *   It **BUYS** the actual future contract.
    *   It **BUYS** the corresponding put option.

## ü§ñ Predictive Analytics Pipeline

We use a LightGBM-based machine learning pipeline to forecast future asset prices, complete with uncertainty estimates.

### Pipeline Execution
To run the full pipeline from data processing to model training and evaluation:
```bash
# 1. Preprocess data and create features
python src/ml_pipeline/preprocess.py

# 2. Train the prediction models
python src/ml_pipeline/train.py

# 3. Evaluate model performance
python src/ml_pipeline/evaluate.py
```

### Feature Engineering
The pipeline automatically engineers a rich feature set from raw time-series data, including:
- **Price-Action Features**: Candlestick properties like body size and wick length.
- **Temporal Features**: Lags and moving averages (SMA/EMA) over various periods.
- **Momentum Indicators**: MACD and other standard momentum metrics.
- **Cross-Asset Features**: Relative strength against the market average.

### Model Architecture & Evaluation
- **Multi-Model Approach**: For each asset, we train four LightGBM models to predict the mean, median (50th percentile), and a 90% prediction interval (using the 5th and 95th percentile models). This interval provides a crucial measure of market uncertainty, which is vital for risk management.
- **Evaluation**: Models are rigorously evaluated for accuracy (MAE, RMSE) and the reliability of their prediction intervals.

## üèóÔ∏è System Architecture

The project's architecture is designed to separate data handling, model training, and trading execution.

**Data Flow:**

1.  **Data Collection**: `src/data_utils/collector.py` captures live data from the `Exchange WebSocket` and saves it into the `data/raw/` directory.
2.  **ML Preprocessing**: `src/ml_pipeline/preprocess.py` reads the raw data, engineers features, and stores the result in `data/processed/`.
3.  **Model Training**: `src/ml_pipeline/train.py` uses the processed data to train the LightGBM models, which are saved in `models/`.
4.  **Trading**:
    *   `src/bots/ml_bot.py` loads the trained models to inform its trading decisions.
    *   `src/bots/main_bot.py` trades based on the put-call parity strategy without relying on the ML models.

This can be visualized as:
```
[Exchange] -> Data Collection -> [Raw Data] -> ML Pipeline -> [Trained Models] -> ML Bot -> [Exchange]
                                                                                Arbitrage Bot -> [Exchange]
```

<br/>

## ‚öôÔ∏è Getting Started

### Prerequisites

Before running the bots, make sure you have all the necessary Python packages installed. You can install them using the provided `requirements.txt` file:

```bash
pip install -r requirements.txt
```

### Configuration

Before running a bot, you may need to configure its parameters.

- **Team Secret**: Set your `TEAM_SECRET` in the corresponding Python file (e.g., `src/bots/main_bot.py`):
  ```python
  TEAM_SECRET = "your-team-secret-here"
  ```
- **Arbitrage Strategy**: In `src/bots/main_bot.py`, you can adjust the `threshold` to define the minimum profit edge required to trigger an arbitrage trade.
- **ML-Based Strategy**: In `src/bots/ml_bot.py`, you can modify parameters related to trade logic, although the core settings are tied to the trained models.

### Running a Bot
To run the primary arbitrage bot:
```bash
python src/bots/main_bot.py
```

## üìÅ Project Structure

The repository is organized into a clean, modular structure:

- **`src/`**: All Python source code.
    - **`bots/`**: Contains the main trading algorithms.
        - `main_bot.py`: The primary bot implementing the Put-Call Parity arbitrage strategy.
        - `ml_bot.py`: An experimental bot that integrates the ML prediction pipeline.
        - `archive/`: Earlier versions of the trading bots for reference.
    - **`data_utils/`**: Scripts for data handling.
        - `collector.py`: Captures and saves live market data.
        - `inspector.py`: A debugging tool to inspect raw WebSocket messages.
    - **`ml_pipeline/`**: The complete machine learning pipeline for price forecasting.
        - `preprocess.py`: Cleans data and engineers features.
        - `train.py`: Trains the LightGBM prediction models.
        - `evaluate.py`: Validates model performance.

- **`data/`**: All project data, separated by type.
    - `raw/`: Raw, unprocessed market data collected from the exchange.
    - `processed/`: Processed data with engineered features, ready for model training.
    - `bad/`: A collection of market data with known anomalies for robust testing.

- **`models/`**: Stores trained model artifacts (e.g., LightGBM models).

- **`notebooks/`**: Jupyter notebooks for interactive development and analysis.
    - `preprocessing.ipynb`: Notebook for developing the data preprocessing steps.
    - `training.ipynb`: Notebook for developing the model training process.

- **`docs/`**: Project documentation.
    - `AlgoTrade_task.pdf`, `api_commented.txt`, `task-statement.md`

- `requirements.txt`: Lists all Python package dependencies.

## ü§ù Contributing

This project was developed for the AlgoTrade 2025 hackathon. Future work could include refining strategies, enhancing ML models, and improving risk management.

## üìÑ License

Developed by Team El Clasico for the AlgoTrade 2025 competition.

---

*Built with ‚ù§Ô∏è for algorithmic trading excellence*